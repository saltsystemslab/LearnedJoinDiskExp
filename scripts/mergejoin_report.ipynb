{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['command', 'run', 'dataset', 'op', 'input_size', 'result.checksum',\n",
      "       'result.duration_ns', 'result.duration_sec', 'result.inner_disk_fetch',\n",
      "       'result.inner_disk_fetch_size', 'result.inner_total_bytes_fetched',\n",
      "       'result.outer_disk_fetch', 'result.outer_disk_fetch_size',\n",
      "       'result.outer_total_bytes_fetched', 'spec.algo', 'spec.algo_name',\n",
      "       'spec.common_key', 'spec.index.search', 'spec.index.type',\n",
      "       'spec.inner_table', 'spec.key_size', 'spec.key_type',\n",
      "       'spec.load_sstable_in_mem', 'spec.name', 'spec.num_threads',\n",
      "       'spec.outer_table', 'spec.result_path', 'spec.value_size',\n",
      "       'spec.write_result_to_disk', 'spec.index.leaf_size_in_pages'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import os;\n",
    "import json;\n",
    "import builtins\n",
    "import pandas as pd;\n",
    "import duckdb\n",
    "\n",
    "dir = '/home/chesetti/Repos/learned_merge_cleanup/sponge'\n",
    "threads = [\"1\", \"4\", \"16\", \"32\"]\n",
    "op = ['join', 'merge']\n",
    "datasets = ['fb', 'wiki', 'uniform_dense', 'uniform_sparse', 'normal', 'lognormal', 'osm', 'books']\n",
    "test_results = []\n",
    "for op in op:\n",
    "    for dataset in datasets:\n",
    "        for thread in threads:\n",
    "            testdir = os.path.join(dir, \"_\".join([op, dataset, thread]), \"outputs\", \"results\")\n",
    "            if (not os.path.exists(testdir)):\n",
    "                continue\n",
    "            rundirs = os.listdir(testdir)\n",
    "            for rundir in rundirs:\n",
    "                for test_result_file in os.listdir(os.path.join(testdir, rundir)):\n",
    "                    json_file = builtins.open(os.path.join(testdir, rundir, test_result_file))\n",
    "                    test_result = json.load(json_file)\n",
    "                    test_result['run'] = rundir\n",
    "                    test_result['dataset'] = dataset\n",
    "                    test_result['op'] = op\n",
    "                    if dataset == 'osm' or dataset == 'books':\n",
    "                        test_result['input_size'] = 800_000_000 \n",
    "                    else:\n",
    "                        test_result['input_size'] = 200_000_000 \n",
    "                    test_results.append(test_result)\n",
    "                    json_file.close()\n",
    "test_dataframe = pd.json_normalize(test_results)\n",
    "print(test_dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>input_size</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>threads</th>\n",
       "      <th>ratio</th>\n",
       "      <th>op</th>\n",
       "      <th>algo</th>\n",
       "      <th>index_name</th>\n",
       "      <th>index_type</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>inner_bytes_fetched</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fb</td>\n",
       "      <td>200000000</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>join</td>\n",
       "      <td>pgm2048</td>\n",
       "      <td>pgm2048</td>\n",
       "      <td>PGM</td>\n",
       "      <td>2048</td>\n",
       "      <td>0.929766</td>\n",
       "      <td>3199258768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fb</td>\n",
       "      <td>200000000</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>join</td>\n",
       "      <td>pgm256</td>\n",
       "      <td>pgm256</td>\n",
       "      <td>PGM</td>\n",
       "      <td>256</td>\n",
       "      <td>1.404009</td>\n",
       "      <td>3183752272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fb</td>\n",
       "      <td>200000000</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>join</td>\n",
       "      <td>btree2048</td>\n",
       "      <td>btree2048</td>\n",
       "      <td>BTREE</td>\n",
       "      <td>2048</td>\n",
       "      <td>1.038459</td>\n",
       "      <td>3200000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fb</td>\n",
       "      <td>200000000</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>join</td>\n",
       "      <td>hashJoin</td>\n",
       "      <td>hashJoin</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>36.665862</td>\n",
       "      <td>3200000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fb</td>\n",
       "      <td>200000000</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>join</td>\n",
       "      <td>hashJoin</td>\n",
       "      <td>hashJoin</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>39.843666</td>\n",
       "      <td>3200000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5515</th>\n",
       "      <td>books</td>\n",
       "      <td>800000000</td>\n",
       "      <td>REAL</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>merge</td>\n",
       "      <td>btree256</td>\n",
       "      <td>btree256</td>\n",
       "      <td>BTREE</td>\n",
       "      <td>256</td>\n",
       "      <td>10.590426</td>\n",
       "      <td>12800065184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5516</th>\n",
       "      <td>books</td>\n",
       "      <td>800000000</td>\n",
       "      <td>REAL</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>merge</td>\n",
       "      <td>btree1024</td>\n",
       "      <td>btree1024</td>\n",
       "      <td>BTREE</td>\n",
       "      <td>1024</td>\n",
       "      <td>10.417381</td>\n",
       "      <td>12800062688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5517</th>\n",
       "      <td>books</td>\n",
       "      <td>800000000</td>\n",
       "      <td>REAL</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>merge</td>\n",
       "      <td>pgm2048</td>\n",
       "      <td>pgm2048</td>\n",
       "      <td>PGM</td>\n",
       "      <td>2048</td>\n",
       "      <td>10.289159</td>\n",
       "      <td>12800065184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5518</th>\n",
       "      <td>books</td>\n",
       "      <td>800000000</td>\n",
       "      <td>REAL</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>merge</td>\n",
       "      <td>pgm1024</td>\n",
       "      <td>pgm1024</td>\n",
       "      <td>PGM</td>\n",
       "      <td>1024</td>\n",
       "      <td>10.988311</td>\n",
       "      <td>12800062688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5519</th>\n",
       "      <td>books</td>\n",
       "      <td>800000000</td>\n",
       "      <td>REAL</td>\n",
       "      <td>32</td>\n",
       "      <td>100</td>\n",
       "      <td>merge</td>\n",
       "      <td>standard_merge</td>\n",
       "      <td>standard_merge</td>\n",
       "      <td>SM</td>\n",
       "      <td>0</td>\n",
       "      <td>10.283070</td>\n",
       "      <td>12800065184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5520 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset  input_size dataset_type  threads  ratio     op            algo  \\\n",
       "0         fb   200000000         REAL        1     50   join         pgm2048   \n",
       "1         fb   200000000         REAL        1     30   join          pgm256   \n",
       "2         fb   200000000         REAL        1     70   join       btree2048   \n",
       "3         fb   200000000         REAL        1     80   join        hashJoin   \n",
       "4         fb   200000000         REAL        1     70   join        hashJoin   \n",
       "...      ...         ...          ...      ...    ...    ...             ...   \n",
       "5515   books   800000000         REAL       32    100  merge        btree256   \n",
       "5516   books   800000000         REAL       32     50  merge       btree1024   \n",
       "5517   books   800000000         REAL       32    100  merge         pgm2048   \n",
       "5518   books   800000000         REAL       32     50  merge         pgm1024   \n",
       "5519   books   800000000         REAL       32    100  merge  standard_merge   \n",
       "\n",
       "          index_name index_type  epsilon  duration_sec  inner_bytes_fetched  \n",
       "0            pgm2048        PGM     2048      0.929766           3199258768  \n",
       "1             pgm256        PGM      256      1.404009           3183752272  \n",
       "2          btree2048      BTREE     2048      1.038459           3200000000  \n",
       "3           hashJoin         NA        0     36.665862           3200000000  \n",
       "4           hashJoin         NA        0     39.843666           3200000000  \n",
       "...              ...        ...      ...           ...                  ...  \n",
       "5515        btree256      BTREE      256     10.590426          12800065184  \n",
       "5516       btree1024      BTREE     1024     10.417381          12800062688  \n",
       "5517         pgm2048        PGM     2048     10.289159          12800065184  \n",
       "5518         pgm1024        PGM     1024     10.988311          12800062688  \n",
       "5519  standard_merge         SM        0     10.283070          12800065184  \n",
       "\n",
       "[5520 rows x 12 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = {\n",
    "    \"pgm256\": 256,\n",
    "    \"pgm1024\": 1024,\n",
    "    \"pgm2048\": 2048,\n",
    "    \"btree256\": 256,\n",
    "    \"btree512\": 512,\n",
    "    \"btree1024\": 1024,\n",
    "    \"btree1048\": 1024,\n",
    "    \"btree2048\": 2048,\n",
    "    \"hashjoin\": 0,\n",
    "    \"btree\": 0,\n",
    "    \"sj\": 0,\n",
    "    \"sj2\": 0,\n",
    "    \"standard_merge\": 0\n",
    "};\n",
    "indexType = {\n",
    "    \"pgm64\": \"PGM\",\n",
    "    \"pgm128\": \"PGM\",\n",
    "    \"pgm256\": \"PGM\",\n",
    "    \"pgm512\": \"PGM\",\n",
    "    \"pgm1024\": \"PGM\",\n",
    "    \"pgm2048\": \"PGM\",\n",
    "    \"btree256\": \"BTREE\",\n",
    "    \"btree512\": \"BTREE\",\n",
    "    \"btree1024\": \"BTREE\",\n",
    "    \"btree1048\": \"BTREE\",\n",
    "    \"btree2048\": \"BTREE\",\n",
    "    \"hashjoin\": \"NA\",\n",
    "    \"btree\": 0,\n",
    "    \"sj\": \"SJ\",\n",
    "    \"sj2\": \"NA\",\n",
    "    \"standard_merge\": \"SM\"\n",
    "};\n",
    "dataset_type = {\n",
    "    \"fb\": \"REAL\",\n",
    "    \"wiki\": \"REAL\",\n",
    "    \"osm\": \"REAL\",\n",
    "    \"books\": \"REAL\",\n",
    "    \"uniform_dense\": \"SYNTHETIC\",\n",
    "    \"uniform_sparse\": \"SYNTHETIC\",\n",
    "    \"normal\": \"SYNTHETIC\",\n",
    "    \"lognormal\": \"SYNTHETIC\",\n",
    "};\n",
    "dataset_display = {\n",
    "    \"fb\": \"fb\",\n",
    "    \"wiki\": \"wiki\",\n",
    "    \"osm\": \"osm\",\n",
    "    \"books\": \"books\",\n",
    "    \"uniform_dense\": \"udense\",\n",
    "    \"uniform_sparse\": \"usparse\",\n",
    "    \"normal\": \"normal\",\n",
    "    \"lognormal\": \"lognormal\",\n",
    "};\n",
    "dataset_order = {\n",
    "    \"fb\": 0,\n",
    "    \"wiki\": 1,\n",
    "    \"osm\": 2,\n",
    "    \"books\": 3,\n",
    "    \"uniform_dense\": 4,\n",
    "    \"uniform_sparse\": 5,\n",
    "    \"normal\": 6,\n",
    "    \"lognormal\": 7,\n",
    "};\n",
    "\n",
    "test_dataframe[\"threads\"] = test_dataframe[\"spec.num_threads\"]\n",
    "test_dataframe[\"duration_sec\"] = test_dataframe[\"result.duration_ns\"] / (1000000000)\n",
    "test_dataframe[\"ratio\"] = test_dataframe[\"spec.common_key\"]\n",
    "test_dataframe[\"algo\"] = test_dataframe[\"spec.algo_name\"]\n",
    "test_dataframe[\"inner_bytes_fetched\"] = test_dataframe[\"result.inner_total_bytes_fetched\"]\n",
    "test_dataframe[\"epsilon\"] = test_dataframe[\"spec.algo_name\"].map(lambda x: epsilon[x.lower()])\n",
    "test_dataframe[\"index_type\"] = test_dataframe[\"spec.algo_name\"].map(lambda x: indexType[x.lower()])\n",
    "test_dataframe[\"index_name\"] = test_dataframe[\"spec.algo_name\"]\n",
    "test_dataframe[\"dataset_type\"] = test_dataframe[\"dataset\"].map(lambda x: dataset_type[x.lower()])\n",
    "\n",
    "results = test_dataframe[[\"dataset\", \"input_size\", \"dataset_type\", \"threads\", \"ratio\", \"op\", \"algo\", \"index_name\", \"index_type\", \"epsilon\", \"duration_sec\", \"inner_bytes_fetched\"]]\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = duckdb.sql(\n",
    "    \" SELECT ratio, epsilon, op, index_type, AVG(duration_sec) as v FROM results \"\n",
    "    \" WHERE threads = 1 AND input_size=200000000 AND op='join' AND index_type!='NA'\"\n",
    "    \" GROUP BY ratio, threads, op, input_size, index_type, epsilon\"\n",
    "    \" ORDER BY ratio, threads, epsilon\"\n",
    ")\n",
    "p = (duckdb.sql(\"PIVOT rows ON index_type,epsilon USING avg(v) GROUP BY ratio ORDER BY RATIO\"))\n",
    "print(p)\n",
    "print(p.df()[['ratio', 'SJ_0', 'BTREE_256', 'BTREE_2048', 'PGM_256', 'PGM_2048']].to_latex(index=False, float_format='%.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = duckdb.sql(\n",
    "    \" SELECT ratio, threads, epsilon, op, index_name, AVG(duration_sec) as v FROM results \"\n",
    "    \" WHERE (ratio=10 or ratio=50 or ratio=100) AND input_size=200000000 AND op='join' AND index_type!='NA'\"\n",
    "    \" AND (index_name='sj' OR epsilon=2048)\"\n",
    "    \" GROUP BY ratio, threads, op, input_size, index_name, epsilon\"\n",
    "    \" ORDER BY ratio, threads, epsilon\"\n",
    ")\n",
    "rows\n",
    "print(duckdb.sql(\"PIVOT rows ON index_name USING AVG(v) GROUP BY threads,ratio ORDER BY threads,ratio\").df().to_latex(index=False, float_format='%.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌───────┬─────────┬────────────────────┬────────────────────┬───┬────────────────────┬─────────┬─────────┬────────┐\n",
      "│ ratio │ BTREE_0 │     BTREE_1024     │     BTREE_2048     │ … │        SM_0        │ SM_1024 │ SM_2048 │ SM_256 │\n",
      "│ int64 │ double  │       double       │       double       │   │       double       │ double  │ double  │ double │\n",
      "├───────┼─────────┼────────────────────┼────────────────────┼───┼────────────────────┼─────────┼─────────┼────────┤\n",
      "│    10 │    NULL │  6.515239308055556 │  6.245084602555556 │ … │  7.306423629777778 │    NULL │    NULL │   NULL │\n",
      "│    20 │    NULL │  4.709472161611111 │  4.545033750666666 │ … │  6.891521441611111 │    NULL │    NULL │   NULL │\n",
      "│    30 │    NULL │ 4.1047253263888885 │ 3.9707228872777782 │ … │  6.734088810944444 │    NULL │    NULL │   NULL │\n",
      "│    40 │    NULL │ 3.7468778676666665 │  3.696040477944444 │ … │  6.635971177333333 │    NULL │    NULL │   NULL │\n",
      "│    50 │    NULL │       3.5406047055 │  3.518445066833333 │ … │  6.622938198222224 │    NULL │    NULL │   NULL │\n",
      "│    60 │    NULL │ 3.4519670198888885 │ 3.3473434158333344 │ … │  6.567717842055555 │    NULL │    NULL │   NULL │\n",
      "│    70 │    NULL │ 3.3257650087222217 │  3.323700884944444 │ … │  6.539584468944444 │    NULL │    NULL │   NULL │\n",
      "│    80 │    NULL │ 3.2371254718333335 │  3.211788345944444 │ … │  6.516234212500001 │    NULL │    NULL │   NULL │\n",
      "│    90 │    NULL │  3.189143784833333 │ 3.1814907173333324 │ … │  6.539199855944445 │    NULL │    NULL │   NULL │\n",
      "│   100 │    NULL │ 3.1749942682777785 │        3.116785359 │ … │ 6.4325288708888895 │    NULL │    NULL │   NULL │\n",
      "├───────┴─────────┴────────────────────┴────────────────────┴───┴────────────────────┴─────────┴─────────┴────────┤\n",
      "│ 10 rows                                                                                    13 columns (8 shown) │\n",
      "└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n",
      "\\begin{tabular}{rrrrrr}\n",
      "\\toprule\n",
      "ratio & SM_0 & BTREE_256 & BTREE_2048 & PGM_256 & PGM_2048 \\\\\n",
      "\\midrule\n",
      "10 & 7.306 & 6.549 & 6.245 & 4.991 & 4.826 \\\\\n",
      "20 & 6.892 & 4.784 & 4.545 & 3.943 & 3.841 \\\\\n",
      "30 & 6.734 & 4.171 & 3.971 & 3.579 & 3.470 \\\\\n",
      "40 & 6.636 & 3.768 & 3.696 & 3.366 & 3.376 \\\\\n",
      "50 & 6.623 & 3.563 & 3.518 & 3.297 & 3.243 \\\\\n",
      "60 & 6.568 & 3.414 & 3.347 & 3.157 & 3.099 \\\\\n",
      "70 & 6.540 & 3.339 & 3.324 & 3.123 & 3.055 \\\\\n",
      "80 & 6.516 & 3.278 & 3.212 & 3.041 & 3.019 \\\\\n",
      "90 & 6.539 & 3.265 & 3.181 & 3.041 & 3.061 \\\\\n",
      "100 & 6.433 & 3.254 & 3.117 & 3.018 & 2.979 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = duckdb.sql(\n",
    "    \" SELECT ratio, epsilon, op, index_type, AVG(duration_sec) as v FROM results \"\n",
    "    \" WHERE threads = 1 AND input_size=200000000 AND op='merge' AND index_type!='NA'\"\n",
    "    \" GROUP BY ratio, threads, op, input_size, index_type, epsilon\"\n",
    "    \" ORDER BY ratio, threads, epsilon\"\n",
    ")\n",
    "p = (duckdb.sql(\"PIVOT rows ON index_type,epsilon USING avg(v) GROUP BY ratio ORDER BY RATIO\"))\n",
    "print(p)\n",
    "print(p.df()[['ratio', 'SM_0', 'BTREE_256', 'BTREE_2048', 'PGM_256', 'PGM_2048']].to_latex(index=False, float_format='%.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\toprule\n",
      "threads & ratio & btree2048 & pgm2048 \\\\\n",
      "\\midrule\n",
      "1 & 10 & 6.245 & 4.826 \\\\\n",
      "1 & 50 & 3.518 & 3.243 \\\\\n",
      "1 & 100 & 3.117 & 2.979 \\\\\n",
      "4 & 10 & 2.702 & 2.647 \\\\\n",
      "4 & 50 & 2.500 & 2.641 \\\\\n",
      "4 & 100 & 2.532 & 2.436 \\\\\n",
      "16 & 10 & 2.722 & 2.705 \\\\\n",
      "16 & 50 & 2.550 & 2.514 \\\\\n",
      "16 & 100 & 2.509 & 2.491 \\\\\n",
      "32 & 10 & 2.900 & 2.949 \\\\\n",
      "32 & 50 & 2.703 & 2.767 \\\\\n",
      "32 & 100 & 2.749 & 2.730 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = duckdb.sql(\n",
    "    \" SELECT ratio, threads, epsilon, op, index_name, AVG(duration_sec) as v FROM results \"\n",
    "    \" WHERE (ratio=10 or ratio=50 or ratio=100) AND input_size=200000000 AND op='merge' AND index_type!='NA'\"\n",
    "    \" AND (index_name='standard_merge' OR epsilon=2048)\"\n",
    "    \" GROUP BY ratio, threads, op, input_size, index_name, epsilon\"\n",
    "    \" ORDER BY ratio, threads, epsilon\"\n",
    ")\n",
    "rows\n",
    "print(duckdb.sql(\"PIVOT rows ON index_name USING AVG(v) GROUP BY threads,ratio ORDER BY threads,ratio\").df().to_latex(index=False, float_format='%.3f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_threads(op, ratio):\n",
    "    rows = duckdb.sql(\n",
    "        \" SELECT ratio,index_name,threads, MEDIAN(duration) as v\"\n",
    "        f\" FROM results where op='{op}' AND ratio={ratio}  AND index_type!='NA'\"\n",
    "        \" GROUP BY dataset, ratio, index_name, threads \"\n",
    "        \" ORDER BY Threads\")\n",
    "    return duckdb.sql('PIVOT rows ON index_name USING MEDIAN(v)')\n",
    "ratios = [10, 50, 100]\n",
    "for ratio in ratios:\n",
    "    for op in ['join', 'merge']:\n",
    "        group_by_threads(op, ratio).df().to_csv('_'.join([op, str(ratio), 'threadStudy.csv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGM Index Memory Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_btree_pgm_memory(epsilon, thread, dataset_type):\n",
    "        where = f\"WHERE threads={thread} AND epsilon={epsilon} AND ratio=10 \"\n",
    "        if dataset_type != \"all\":\n",
    "                where = where + f\" AND dataset_type='{dataset_type}' \"\n",
    "        query = (\"SELECT ratio, threads, algo, index_type, dataset, dataset_type,\"\n",
    "                \"epsilon, MEAN(index_memory) as memory_mean, FROM results \" \n",
    "                + where +\n",
    "                \"GROUP BY (ratio, threads, algo, index_type, epsilon, dataset, dataset_type) \"\n",
    "                \"ORDER BY ratio\")\n",
    "        subresult = duckdb.sql(query)\n",
    "        df = duckdb.sql(\"PIVOT subresult ON index_type USING MEAN(memory_mean) GROUP BY ratio,dataset\").df()\n",
    "        df['REL'] = (df['PGM']) / df['BTREE']\n",
    "        return df\n",
    "\n",
    "def compare_btree_pgm_build_duration(epsilon, thread, dataset_type):\n",
    "        where = f\"WHERE threads={thread} AND epsilon={epsilon} AND ratio=10 \"\n",
    "        if dataset_type != \"all\":\n",
    "                where = where + f\" AND dataset_type='{dataset_type}' \"\n",
    "        query = (\"SELECT ratio, threads, algo, index_type, dataset, dataset_type,\"\n",
    "                \"epsilon, MEAN(index_build_duration) as index_build_duration, FROM results \" \n",
    "                + where +\n",
    "                \"GROUP BY (ratio, threads, algo, index_type, epsilon, dataset, dataset_type) \"\n",
    "                \"ORDER BY ratio\")\n",
    "        subresult = duckdb.sql(query)\n",
    "        df = duckdb.sql(\"PIVOT subresult ON index_type USING MEAN(index_build_duration) GROUP BY ratio,dataset\").df()\n",
    "        df['REL'] = (df['PGM']) / df['BTREE']\n",
    "        return df\n",
    "dict = []\n",
    "dict.append({\n",
    "        'epsilon':256, \n",
    "        'ALL': compare_btree_pgm_memory(epsilon=256, thread=1, dataset_type='all')['REL'].mean(),\n",
    "        'REAL': compare_btree_pgm_memory(epsilon=256, thread=1, dataset_type='REAL')['REL'].mean(),\n",
    "        'SYNTHETIC': compare_btree_pgm_memory(epsilon=256, thread=1, dataset_type='SYNTHETIC')['REL'].mean(),\n",
    "        'ACTUAL': compare_btree_pgm_memory(epsilon=256, thread=1, dataset_type='all')['PGM'].mean(),\n",
    "        })\n",
    "dict.append({\n",
    "        'epsilon':1024, \n",
    "        'ALL': compare_btree_pgm_memory(epsilon=1024, thread=1, dataset_type='all')['REL'].mean(),\n",
    "        'REAL': compare_btree_pgm_memory(epsilon=1024, thread=1, dataset_type='REAL')['REL'].mean(),\n",
    "        'SYNTHETIC': compare_btree_pgm_memory(epsilon=1024, thread=1, dataset_type='SYNTHETIC')['REL'].mean(),\n",
    "        'ACTUAL': compare_btree_pgm_memory(epsilon=1024, thread=1, dataset_type='all')['PGM'].mean(),\n",
    "        })\n",
    "dict.append({\n",
    "        'epsilon': 2048, \n",
    "        'ALL': compare_btree_pgm_memory(epsilon=2048, thread=1, dataset_type='all')['REL'].mean(),\n",
    "        'REAL': compare_btree_pgm_memory(epsilon=2048, thread=1, dataset_type='REAL')['REL'].mean(),\n",
    "        'SYNTHETIC': compare_btree_pgm_memory(epsilon=2048, thread=1, dataset_type='SYNTHETIC')['REL'].mean(),\n",
    "        'ACTUAL': compare_btree_pgm_memory(epsilon=2048, thread=1, dataset_type='all')['PGM'].mean(),\n",
    "        })\n",
    "\n",
    "print(pd.DataFrame(dict).to_latex(index=False))\n",
    "\n",
    "dict = []\n",
    "dict.append({\n",
    "        'epsilon':256, \n",
    "        'ALL': compare_btree_pgm_build_duration(epsilon=256, thread=1, dataset_type='all')['REL'].mean(),\n",
    "        'REAL': compare_btree_pgm_build_duration(epsilon=256, thread=1, dataset_type='REAL')['REL'].mean(),\n",
    "        'SYNTHETIC': compare_btree_pgm_build_duration(epsilon=256, thread=1, dataset_type='SYNTHETIC')['REL'].mean()\n",
    "        })\n",
    "dict.append({\n",
    "        'epsilon':1024, \n",
    "        'ALL': compare_btree_pgm_build_duration(epsilon=1024, thread=1, dataset_type='all')['REL'].mean(),\n",
    "        'REAL': compare_btree_pgm_build_duration(epsilon=1024, thread=1, dataset_type='REAL')['REL'].mean(),\n",
    "        'SYNTHETIC': compare_btree_pgm_build_duration(epsilon=1024, thread=1, dataset_type='SYNTHETIC')['REL'].mean()\n",
    "        })\n",
    "dict.append({\n",
    "        'epsilon': 2048, \n",
    "        'ALL': compare_btree_pgm_build_duration(epsilon=2048, thread=1, dataset_type='all')['REL'].mean(),\n",
    "        'REAL': compare_btree_pgm_build_duration(epsilon=2048, thread=1, dataset_type='REAL')['REL'].mean(),\n",
    "        'SYNTHETIC': compare_btree_pgm_build_duration(epsilon=2048, thread=1, dataset_type='SYNTHETIC')['REL'].mean()\n",
    "        })\n",
    "print(pd.DataFrame(dict).to_latex(index=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PGM vs TREE SJ same epsilon (Duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_btree_pgm_duration(epsilon, thread, dataset, op):\n",
    "        where = f\"WHERE threads={thread} AND op='{op}' AND (epsilon={epsilon} OR algo='sj' OR algo='standard_merge')\"\n",
    "        if dataset != \"all\":\n",
    "                where = where + f\"AND dataset='{dataset}'\"\n",
    "        query = (\"SELECT ratio, threads, algo, index_type,\"\n",
    "                \"epsilon, MEDIAN(duration) as duration_mean, FROM results \" \n",
    "                + where +\n",
    "                \"GROUP BY (ratio, threads, algo, index_type, epsilon) \"\n",
    "                \"ORDER BY ratio\")\n",
    "        subresult = duckdb.sql(query)\n",
    "        print(subresult)\n",
    "        df = duckdb.sql(\"PIVOT subresult ON index_type USING sum(duration_mean) GROUP BY ratio\").df()\n",
    "        print(df)\n",
    "        baseline = 'SJ' if op == 'join' else 'SM'\n",
    "        df['RATIO'] = df['ratio']\n",
    "        if 'PGM' in df.columns:\n",
    "                df['PGM_REL'] = (df['PGM']/ df[baseline])\n",
    "        if 'BTREE' in df.columns:\n",
    "                df['BTREE_REL'] = (df['BTREE']/ df[baseline])\n",
    "        return df\n",
    "\n",
    "def compare_pgm_bytes_fetched(epsilon, thread, dataset, op):\n",
    "        where = f\"WHERE threads={thread} AND op='{op}' AND (epsilon={epsilon} OR algo='sj' OR algo='standard_merge')\"\n",
    "        if dataset != \"all\":\n",
    "                where = where + f\"AND dataset='{dataset}'\"\n",
    "        query = (\"SELECT ratio, threads, algo, index_type,\"\n",
    "                \"epsilon, MEDIAN(inner_bytes_fetched) as median, FROM results \" \n",
    "                + where +\n",
    "                \"GROUP BY (ratio, threads, algo, index_type, epsilon) \"\n",
    "                \"ORDER BY ratio\")\n",
    "        subresult = duckdb.sql(query)\n",
    "        df = duckdb.sql(\"PIVOT subresult ON index_type USING AVG(median) GROUP BY ratio\").df()\n",
    "        baseline = 'SJ' if op == 'join' else 'SM'\n",
    "        df['RATIO'] = df['ratio']\n",
    "        df['BTREE_REL'] = (df['BTREE']/ df[baseline])\n",
    "        df['PGM_REL'] = (df['PGM']/ df[baseline])\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame();\n",
    "df['ratio'] = (compare_btree_pgm_duration(epsilon=2048, thread=1, dataset='all', op='join')['RATIO'])\n",
    "df['t=1'] = (compare_btree_pgm_duration(epsilon=2048, thread=1, dataset='all', op='join')['PGM_REL'])\n",
    "df['t=4'] = (compare_btree_pgm_duration(epsilon=2048, thread=4, dataset='all', op='join')['PGM_REL'])\n",
    "df['t=16'] = (compare_btree_pgm_duration(epsilon=2048, thread=16, dataset='all', op='join')['PGM_REL'])\n",
    "df['t=32'] = (compare_btree_pgm_duration(epsilon=2048, thread=32, dataset='all', op='join')['PGM_REL'])\n",
    "print(df.to_latex(index=False))\n",
    "\n",
    "df = pd.DataFrame();\n",
    "df['ratio'] = (compare_pgm_bytes_fetched(epsilon=2048, thread=1, dataset='all', op='join')['RATIO'])\n",
    "df['t=1'] = (compare_pgm_bytes_fetched(epsilon=2048, thread=1, dataset='all', op='join')['PGM_REL'])\n",
    "df['t=4'] = (compare_pgm_bytes_fetched(epsilon=2048, thread=4, dataset='all', op='join')['PGM_REL'])\n",
    "df['t=16'] = (compare_pgm_bytes_fetched(epsilon=2048, thread=16, dataset='all', op='join')['PGM_REL'])\n",
    "df['t=32'] = (compare_pgm_bytes_fetched(epsilon=2048, thread=32, dataset='all', op='join')['PGM_REL'])\n",
    "print(df.to_latex(index=False))\n",
    "\n",
    "df = pd.DataFrame();\n",
    "df['ratio'] = (compare_btree_pgm_duration(epsilon=2048, thread=1, dataset='all', op='merge')['RATIO'])\n",
    "df['t=1'] = (compare_btree_pgm_duration(epsilon=2048, thread=1, dataset='all', op='merge')['PGM_REL'])\n",
    "df['t=4'] = (compare_btree_pgm_duration(epsilon=2048, thread=4, dataset='all', op='merge')['PGM_REL'])\n",
    "df['t=16'] = (compare_btree_pgm_duration(epsilon=2048, thread=16, dataset='all', op='merge')['PGM_REL'])\n",
    "df['t=32'] = (compare_btree_pgm_duration(epsilon=2048, thread=32, dataset='all', op='merge')['PGM_REL'])\n",
    "print(df.to_latex(index=False))\n",
    "\n",
    "df = pd.DataFrame();\n",
    "df['ratio'] = (compare_pgm_bytes_fetched(epsilon=2048, thread=1, dataset='all', op='merge')['RATIO'])\n",
    "df['t=1'] = (compare_pgm_bytes_fetched(epsilon=2048, thread=1, dataset='all', op='merge')['PGM_REL'])\n",
    "df['t=4'] = (compare_pgm_bytes_fetched(epsilon=2048, thread=4, dataset='all', op='merge')['PGM_REL'])\n",
    "df['t=16'] = (compare_pgm_bytes_fetched(epsilon=2048, thread=16, dataset='all', op='merge')['PGM_REL'])\n",
    "df['t=32'] = (compare_pgm_bytes_fetched(epsilon=2048, thread=32, dataset='all', op='merge')['PGM_REL'])\n",
    "print(df.to_latex(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = compare_btree_pgm_duration\n",
    "df = []\n",
    "df.append({'epsilon': 256, 'index': 'PGM', 't': 1, 'value': compare_btree_pgm_duration(epsilon=256, thread=1, dataset='all', op='join')['PGM_REL'].mean()})\n",
    "df.append({'epsilon': 1024, 'index': 'PGM', 't': 1, 'value': compare_btree_pgm_duration(epsilon=1024, thread=1, dataset='all', op='join')['PGM_REL'].mean()})\n",
    "df.append({'epsilon': 2048, 'index': 'PGM', 't': 1, 'value': compare_btree_pgm_duration(epsilon=2048, thread=1, dataset='all', op='join')['PGM_REL'].mean()})\n",
    "\n",
    "df.append({'epsilon': 256, 'index': 'BTREE', 't': 1, 'value': compare_btree_pgm_duration(epsilon=256, thread=1, dataset='all', op='join')['BTREE_REL'].mean()})\n",
    "df.append({'epsilon': 1024, 'index': 'BTREE', 't': 1, 'value': compare_btree_pgm_duration(epsilon=1024, thread=1, dataset='all', op='join')['BTREE_REL'].mean()})\n",
    "df.append({'epsilon': 2048, 'index': 'BTREE', 't': 1, 'value': compare_btree_pgm_duration(epsilon=2048, thread=1, dataset='all', op='join')['BTREE_REL'].mean()})\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "print(df.pivot(index='epsilon', columns='index', values='value').to_latex())\n",
    "\n",
    "f = compare_btree_pgm_duration\n",
    "df = []\n",
    "df.append({'epsilon': 256, 'index': 'PGM', 't': 1, 'value': compare_btree_pgm_duration(epsilon=256, thread=1, dataset='all', op='merge')['PGM_REL'].mean()})\n",
    "df.append({'epsilon': 2048, 'index': 'PGM', 't': 1, 'value': compare_btree_pgm_duration(epsilon=2048, thread=1, dataset='all', op='merge')['PGM_REL'].mean()})\n",
    "\n",
    "df.append({'epsilon': 256, 'index': 'BTREE', 't': 1, 'value': compare_btree_pgm_duration(epsilon=256, thread=1, dataset='all', op='merge')['BTREE_REL'].mean()})\n",
    "df.append({'epsilon': 2048, 'index': 'BTREE', 't': 1, 'value': compare_btree_pgm_duration(epsilon=2048, thread=1, dataset='all', op='merge')['BTREE_REL'].mean()})\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "print(df.pivot(index='epsilon', columns='index', values='value').to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = compare_btree_pgm_duration\n",
    "df = []\n",
    "\n",
    "df.append({'epsilon': 2048, 'index': 'PGM', 't': 1, 'value': compare_btree_pgm_duration(epsilon=2048, thread=1, dataset='all', op='merge')['PGM_REL'].mean()})\n",
    "\n",
    "df.append({'epsilon': 512, 'index': 'PGM', 't': 1, 'value': compare_btree_pgm_duration(epsilon=512, thread=1, dataset='all', op='merge')['PGM_REL'].mean()})\n",
    "df.append({'epsilon': 2048, 'index': 'BTREE', 't': 1, 'value': compare_btree_pgm_duration(epsilon=2048, thread=1, dataset='all', op='merge')['BTREE_REL'].mean()})\n",
    "df.append({'epsilon': 256, 'index': 'BTREE', 't': 1, 'value': compare_btree_pgm_duration(epsilon=256, thread=1, dataset='all', op='merge')['BTREE_REL'].mean()})\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "print(df.pivot(index='epsilon', columns='index', values='value').to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
